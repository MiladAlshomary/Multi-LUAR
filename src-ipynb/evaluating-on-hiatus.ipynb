{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98add410-2397-4016-a873-968289bfacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39967725-2cd2-4da3-8e97-79546fd8626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f92b73-a37d-443b-90ac-e6f131417e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/swordfish-pool2/milad/conda-envs/huggingface-tlr/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.utilities.mluar_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68524a3c-6085-46ff-8639-45985108e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import pickle as pkl\n",
    "import tabulate\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5979105-cb91-4941-a198-4cf18e2f5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utilities.mluar_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61abce0-9a44-4819-9999-27578cc60775",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_LUAR_PATH =  \"/mnt/swordfish-pool2/milad/multi-luar-reddit-model/\"\n",
    "LUAR_PATH =  \"/mnt/swordfish-pool2/nikhil/LUAR/pretrained_weights/LUAR-MUD/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc7098f-1fd3-42e8-bcec-af94d8211d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "multiluar_model = AutoModel.from_pretrained(MULTI_LUAR_PATH, trust_remote_code=True)\n",
    "luar_model = AutoModel.from_pretrained(LUAR_PATH, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rrivera1849/LUAR-MUD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff01ec9-1b79-494c-864e-7d8052a14792",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2dc96a-a934-4609-a704-3b91aa8d4036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.1/data/HRS1_english_long_sample-0_perGenre-HRS1.1_TA2_input\n",
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.2/data/HRS1_english_long_sample-0_perGenre-HRS1.2_TA2_input\n",
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.3/data/HRS1_english_long_sample-0_perGenre-HRS1.3_TA2_input\n",
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.4/data/HRS1_english_long_sample-0_perGenre-HRS1.4_TA2_input\n"
     ]
    }
   ],
   "source": [
    "data_embeddings = {}\n",
    "max_seq_length = 512\n",
    "domain_path_temp = data_path + '/HRS1_english_long_sample-0_perGenre-{}/{}/HRS1_english_long_sample-0_perGenre-{}_TA2'\n",
    "\n",
    "for domain in ['HRS1.1', 'HRS1.2', 'HRS1.3', 'HRS1.4']:\n",
    "    # Load data\n",
    "    domain_data_path = domain_path_temp.format(domain, 'data', domain) + '_input'\n",
    "    domain_groundtruth_path = domain_path_temp.format(domain, 'groundtruth', domain) \n",
    "    hiatus_data, _, _ = load_aa_data(domain_data_path, domain_groundtruth_path)\n",
    "\n",
    "    # keep authors with only more than one text\n",
    "    authors_with_multiple_texts = [x[0] for x in hiatus_data.authorID.value_counts().to_dict().items() if x[1] > 1]\n",
    "    hiatus_data = hiatus_data[hiatus_data.authorID.isin(authors_with_multiple_texts)]\n",
    "\n",
    "    # Embed data using m-luar and luar\n",
    "    hiatus_data_texts = hiatus_data.fullText.tolist()\n",
    "    hiatus_mluar_data_embeddings,_ = get_luar_embeddings(hiatus_data_texts, multiluar_model, tokenizer, max_length=max_seq_length, batch_size=1, is_multi_luar=True)\n",
    "    hiatus_luar_data_embeddings,_  = get_luar_embeddings(hiatus_data_texts, luar_model, tokenizer, max_length=max_seq_length, batch_size=1)\n",
    "    hiatus_luar_data_embeddings    = [e.unsqueeze(0) for e in hiatus_luar_data_embeddings]\n",
    "    \n",
    "    data_embeddings[domain] = {'LUAR': hiatus_luar_data_embeddings, 'MLUAR': hiatus_mluar_data_embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd903d1a-6ac7-438c-8270-adb87648e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(data_embeddings, open('./hiatus_data_embedded.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "504971d4-5580-4836-b06b-26404421660d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.1/data/HRS1_english_long_sample-0_perGenre-HRS1.1_TA2_input\n",
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.2/data/HRS1_english_long_sample-0_perGenre-HRS1.2_TA2_input\n",
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.3/data/HRS1_english_long_sample-0_perGenre-HRS1.3_TA2_input\n",
      "Loading:  /mnt/swordfish-pool2/milad/hiatus-data/HRS_evaluation_samples/HRS1_english_long/TA2/HRS1_english_long_sample-0_perGenre-HRS1.4/data/HRS1_english_long_sample-0_perGenre-HRS1.4_TA2_input\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for domain in ['HRS1.1', 'HRS1.2', 'HRS1.3', 'HRS1.4']:\n",
    "\n",
    "    # Load data\n",
    "    domain_data_path = domain_path_temp.format(domain, 'data', domain) + '_input'\n",
    "    domain_groundtruth_path = domain_path_temp.format(domain, 'groundtruth', domain) \n",
    "    hiatus_data, _, _ = load_aa_data(domain_data_path, domain_groundtruth_path)\n",
    "    # keep authors with only more than one text\n",
    "    authors_with_multiple_texts = [x[0] for x in hiatus_data.authorID.value_counts().to_dict().items() if x[1] > 1]\n",
    "    hiatus_data = hiatus_data[hiatus_data.authorID.isin(authors_with_multiple_texts)]\n",
    "    \n",
    "    labels = hiatus_data.authorID.tolist()\n",
    "\n",
    "    #Load embeddings from the saved dictionary\n",
    "    hiatus_mluar_data_embeddings = data_embeddings[domain]['MLUAR']\n",
    "    hiatus_luar_data_embeddings  = data_embeddings[domain]['LUAR']\n",
    "\n",
    "    luar_sims = compute_similarities(hiatus_luar_data_embeddings, hiatus_luar_data_embeddings)\n",
    "    \n",
    "    labels_matrix = np.array([[int(x == y) for y in labels] for x in labels])\n",
    "    luar_eer, luar_mrr   = eer(luar_sims, labels_matrix), compute_mrr(luar_sims, labels)\n",
    "\n",
    "    results.append(['+++', '+++', '+++', '+++'])\n",
    "    results.append(['LUAR', domain, luar_eer, luar_mrr])\n",
    "\n",
    "    muti_luar_layers_sims = np.stack([compute_similarities(hiatus_mluar_data_embeddings, hiatus_mluar_data_embeddings, layer=i) for i in range(7)])\n",
    "    \n",
    "    muti_luar_layers_sims_ablated = np.mean(muti_luar_layers_sims, 0)\n",
    "    mluar_eer, mluar_mrr = eer(muti_luar_layers_sims_ablated, labels_matrix), compute_mrr(muti_luar_layers_sims_ablated, labels)\n",
    "    results.append(['MLUAR', domain, mluar_eer, mluar_mrr])\n",
    "    results.append(['--', '--', '--', '--'])\n",
    "    \n",
    "    # Ablation study\n",
    "    for layers in [[0], [1,2], [3,4], [5,6]]:\n",
    "        selector = [i for i in range(muti_luar_layers_sims.shape[0]) if i in layers]\n",
    "        #print(selector)\n",
    "        muti_luar_layers_sims_ablated = np.mean(muti_luar_layers_sims[selector, :, :], 0)\n",
    "        mluar_eer, mluar_mrr = eer(muti_luar_layers_sims_ablated, labels_matrix), compute_mrr(muti_luar_layers_sims_ablated, labels)\n",
    "        results.append(['MLUAR/{}'.format(layers), domain, mluar_eer, mluar_mrr])\n",
    "        #results.append(['--', '--', '--', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29eea14e-c98d-4329-b497-12fc19a4ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Domain    EER    MRR\n",
      "------------  --------  -----  -----\n",
      "+++           +++       +++    +++\n",
      "LUAR          HRS1.1    0.098  0.192\n",
      "MLUAR         HRS1.1    0.118  0.174\n",
      "--            --        --     --\n",
      "MLUAR/[0]     HRS1.1    0.123  0.139\n",
      "MLUAR/[1, 2]  HRS1.1    0.121  0.164\n",
      "MLUAR/[3, 4]  HRS1.1    0.118  0.169\n",
      "MLUAR/[5, 6]  HRS1.1    0.106  0.169\n",
      "+++           +++       +++    +++\n",
      "LUAR          HRS1.2    0.1    0.187\n",
      "MLUAR         HRS1.2    0.12   0.17\n",
      "--            --        --     --\n",
      "MLUAR/[0]     HRS1.2    0.125  0.137\n",
      "MLUAR/[1, 2]  HRS1.2    0.123  0.162\n",
      "MLUAR/[3, 4]  HRS1.2    0.119  0.166\n",
      "MLUAR/[5, 6]  HRS1.2    0.108  0.166\n",
      "+++           +++       +++    +++\n",
      "LUAR          HRS1.3    0.1    0.185\n",
      "MLUAR         HRS1.3    0.12   0.166\n",
      "--            --        --     --\n",
      "MLUAR/[0]     HRS1.3    0.125  0.133\n",
      "MLUAR/[1, 2]  HRS1.3    0.121  0.159\n",
      "MLUAR/[3, 4]  HRS1.3    0.119  0.162\n",
      "MLUAR/[5, 6]  HRS1.3    0.109  0.162\n",
      "+++           +++       +++    +++\n",
      "LUAR          HRS1.4    0.105  0.187\n",
      "MLUAR         HRS1.4    0.124  0.167\n",
      "--            --        --     --\n",
      "MLUAR/[0]     HRS1.4    0.132  0.133\n",
      "MLUAR/[1, 2]  HRS1.4    0.126  0.158\n",
      "MLUAR/[3, 4]  HRS1.4    0.124  0.163\n",
      "MLUAR/[5, 6]  HRS1.4    0.114  0.162\n"
     ]
    }
   ],
   "source": [
    "print(tabulate.tabulate(results, headers=['Domain', 'EER', 'MRR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d39ae4-2bc3-4df7-be81-c29434d95a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
